---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

Krystal Li KML3695

### Introduction 

I chose this data solely because I am a Starbucks fan - I found this data on the Vincentarelbundock website, and chose to go with it because I thought it would be interesting to analyze the different foods at Starbucks. The variables measured are as follows: item, calories, fat, carb, fiber, protein, and type. I made a separate categorical column called highcals, that denotes whether the item is high in calories or not, based on if it was higher than the mean of the calories. There are 77 observations, 48 of which are high calories and 29 of which are not. 

```{R}
library(tidyverse)
starbucks <- read_csv("~/project2/starbucks.csv")
starbucks <- starbucks %>% mutate(highcals = ifelse(calories>mean(calories), "true", "false"))
starbucks <- starbucks %>% select(item:highcals)
starbucks %>% select(highcals) %>% filter(highcals=="true") %>% summarize(n=n())
starbucks %>% select(highcals) %>% filter(highcals=="false") %>% summarize(n=n())
```

### Cluster Analysis

```{R}
library(cluster)

sil_width<-vector()
clust_dat <- starbucks %>% select(calories,fat,carb,fiber,protein)
for(i in 2:10){  
kms <- kmeans(clust_dat,centers=i) 
sil <- silhouette(kms$cluster,dist(clust_dat)) 
sil_width[i]<-mean(sil[,3])
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

starbucks_pam <-clust_dat %>% pam(k=3)
starbucks_pam

plot(starbucks_pam, which=2)

starbucks%>%slice(starbucks_pam$id.med)

library(GGally)
starbucks%>%mutate(cluster=as.factor(starbucks_pam$clustering))%>%
ggpairs(columns = 2:6,aes(color=cluster))


```

I clustered based on my numeric variables, which included calories, fat, carb, fiber, and protein (measured in terms of how many calories and then how many grams of everything else).The best number to use to cluster for PAM was 3, and that was when the silhouette width was at its highest as well. This was found by using the silhouette width (we choose the highest point as stated before). The width was around 0.79, compared to the average of 0.58, which indicated that a strong structure had been found. After using both the plots I ran the ggpairs to see the correlation, and the data indicated that the highest correlation was between fat and calories (0.759), while the lowest correlation was carb and protein (-0.583). I also then found the final medoids that are most representative of their cluster

The purpose of clustering is to be able to group our data by patterns, and then to group them by those patterns. We can see then what features appear together, and what characterizes a group within the data frame. The 3 medoids that we see in the final slice are Roasted Vegetable Panini, Maple Oat Pecan Scone, and the Tiramisu Cake Pop, which means that those 3 food items are most representative of the final groups that we clustered. 
    
### Dimensionality Reduction with PCA

```{R}

starbucks_num <- starbucks%>%select(calories:protein)
starbucks_pca<-princomp(starbucks_num,cor=T)
starbucks_pca
summary(starbucks_pca, loadings=T)
starbucks_pca$scores %>% cor() %>% round(5)

starbucks_pcadf <- data.frame(starbucks_pca$scores)
pcaggplot <- data.frame(Name = starbucks_pcadf, PC1 = starbucks_pca$scores[, 1], PC2 = starbucks_pca$scores[, 2])
pcaggplot %>% ggplot(aes(PC1, PC2)) + 
geom_point(color="magenta") 

plot(starbucks_pam, which=1)
```

I performed PCA on my data, using the numeric values (calories,carb,fat,fiber,and protein). If we look at just PC1 and PC2, we cand determine the following: for PC1, that a high score would mean a high score in all the categories (calories, fat, carb, fiber, and protien). For PC2, a high score would mean a high score in calories, fat, carb, but a low score in fiber and protein. I also then ran a scores matrix and rounded it to demonstrate that the 6 principal components were unrelated to each other. The PCA reveals the underlying structure of our data, and the graph between PC1 and PC2 demonstrates that there not necessarily a direct relationship involved. To find the amount that the PCs contribute to the total variance, we run a plot that tells us the amount of point variability explained by the 2 PCs, which is 72.38%. 

###  Linear Classifier

```{R}
fit <-glm(highcals=="true" ~ carb + fiber + protein, data=starbucks, family="binomial")
score<-predict(fit,type="response")
class_diag(score,starbucks$highcals,positive="true")
```
For logistic regression, I could not use calories or fat because they were both perfect predictors. From here, we can see that the AUC is 0.7963, which means that the model is doing ok - not great, but not horrible. (According to Dr.Woodward's slides, a 0.7963 would be the equivalent of a C+, almost a B).

```{R}
k = 10
data <- sample_frac(starbucks)  
folds <- rep(1:k, length.out = nrow(data))

diags <- NULL

i = 1
for (i in 1:k) {
train <- data[folds != i, ]
test <- data[folds == i, ]
truth <- test$highcals
fit <- glm(highcals=="true" ~ carb + fiber + protein, data=starbucks, family="binomial")
probs <- predict(fit, newdata = test, type = "response")
diags <- rbind(diags, class_diag(probs, truth, positive = "true"))
}
summarize_all(diags, mean)
```

We can see here that our model is doing better now that we have ran it with a k-fold CV. It is at a 0.8211, which is also not a superb score, but an improvement from 0.7963. It is not showing signs of overfitting because it is improving when we run it with a k-fold (which means that we are randomly splitting up the data into subsets, which shows us if our data is overfitting if the k-fold score is lower).

### Non-Parametric Classifier

```{R}
library(caret)
knn_fit <- knn3(highcals == "true" ~ carb + fiber + protein, data = starbucks)
prob_knn <- predict(knn_fit, newdata = starbucks)
class_diag(prob_knn[, 2], starbucks$highcals, positive = "true")
```

```{R}
k = 10
data <- sample_frac(starbucks) 
folds <- rep(1:k, length.out = nrow(data))  
diags <- NULL
i = 1
for (i in 1:k) {
train <- data[folds != i, ]
test <- data[folds == i, ]
truth <- test$highcals
fit <- knn3(highcals == "true" ~ carb + fiber + protein, data = starbucks)
probs <- predict(fit, newdata = test)[, 2]
diags <- rbind(diags, class_diag(probs, truth, positive = "true"))
}
summarize_all(diags, mean)

library(rpart); library(rpart.plot)
fit <-rpart(calories~ fat + carb + fiber + protein + type, data=starbucks)
rpart.plot(fit)
```

Discussion


### Regression/Numeric Prediction

```{R}
fit1<-lm(calories~ fat + carb + fiber + protein + type, data=starbucks)
yhat<-predict(fit1)
mean((starbucks$calories-yhat)^2)

k=5 
data<-starbucks[sample(nrow(starbucks)),] 
folds<-cut(seq(1:nrow(starbucks)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
fit<-lm(calories~ fat + carb + fiber + protein + type, data=starbucks)
yhat<-predict(fit,newdata=test)
diags<-mean((starbucks$calories-yhat)^2) 
}

cv <- trainControl(method="cv", number = 5, classProbs = T, savePredictions = T)
fit <- train(calories~ fat + carb + fiber + protein + type, data=starbucks, trControl=cv, method="knn")
fit$results$RMSE^2
```

```{R}
# cross-validation of regression model here
```

Discussion

### Python 

```{R}
library(reticulate)

```

```{python}

```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




