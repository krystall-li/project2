---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

Krystal Li KML3695

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

```{R}
library(tidyverse)
starbucks <- read_csv("~/project2/starbucks.csv")
starbucks <- starbucks %>% mutate(highcals = ifelse(calories>mean(calories), "true", "false"))
starbucks <- starbucks %>% select(item:highcals)
```



### Cluster Analysis

```{R}
library(cluster)

sil_width<-vector()
clust_dat <- starbucks %>% select(calories,fat,carb,fiber,protein)
for(i in 2:10){  
kms <- kmeans(clust_dat,centers=i) 
sil <- silhouette(kms$cluster,dist(clust_dat)) 
sil_width[i]<-mean(sil[,3])
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

starbucks_pam <-clust_dat %>% pam(k=3)
starbucks_pam

plot(starbucks_pam, which=2)

starbucks%>%slice(starbucks_pam$id.med)

library(GGally)
starbucks%>%mutate(cluster=as.factor(starbucks_pam$clustering))%>%
ggpairs(columns = 2:6,aes(color=cluster))


```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
starbucks_num <- starbucks%>%select(calories:protein)
starbucks_pca<-princomp(starbucks_num,cor=T)
starbucks_pca
summary(starbucks_pca, loadings=T)
starbucks_pca$scores %>% cor() %>% round(5)

starbucks_pcadf <- data.frame(starbucks_pca$scores)
pcaggplot <- data.frame(Name = starbucks_pcadf, PC1 = starbucks_pca$scores[, 1], PC2 = starbucks_pca$scores[, 2])
pcaggplot %>% ggplot(aes(PC1, PC2)) + 
geom_point(color="magenta") 
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
fit <-glm(highcals=="true" ~ carb + fiber + protein, data=starbucks, family="binomial")
score<-predict(fit,type="response")
class_diag(score,starbucks$highcals,positive="true")
```
for logistic regression, could not use calories or fat because perfect predictors


```{R}
k = 10
data <- sample_frac(starbucks)  
folds <- rep(1:k, length.out = nrow(data))

diags <- NULL

i = 1
for (i in 1:k) {
train <- data[folds != i, ]
test <- data[folds == i, ]
truth <- test$highcals
fit <- glm(highcals=="true" ~ carb + fiber + protein, data=starbucks, family="binomial")
probs <- predict(fit, newdata = test, type = "response")
diags <- rbind(diags, class_diag(probs, truth, positive = "true"))
}
summarize_all(diags, mean)
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
```

```{R}
# cross-validation of np classifier here
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
```

```{R}
# cross-validation of regression model here
```

Discussion

### Python 

```{R}
library(reticulate)
```

```{python}
# python code here
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




